FewSum(
  (_embds): TransformerEmbeddings(
    (embs): Embedding(33455, 390)
    (dropout): Dropout(p=0.1, inplace=False)
    (pos_enc): Embedding(112, 10)
  )
  (_tr_stack): TransformerStack(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (mem_attn): MultiheadAttention(
          (out_proj): Linear(in_features=400, out_features=400, bias=True)
        )
        (linear1): Linear(in_features=400, out_features=1000, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1000, out_features=400, bias=True)
        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)
    (inp_proj): Sequential(
      (lin): Linear(in_features=409, out_features=400, bias=True)
    )
  )
  (_dec_ffnn): Sequential(
    (lin): Linear(in_features=400, out_features=390, bias=True)
    (emb): TransformerEmbeddings(
      (embs): Embedding(33455, 390)
      (dropout): Dropout(p=0.1, inplace=False)
      (pos_enc): Embedding(112, 10)
    )
  )
  (plugin): Plugin(
    (tr_stack): TransformerStack(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (mem_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (linear1): Linear(in_features=30, out_features=20, bias=True)
          (dropout): Dropout(p=0.4, inplace=False)
          (linear2): Linear(in_features=20, out_features=30, bias=True)
          (norm1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.4, inplace=False)
          (dropout2): Dropout(p=0.4, inplace=False)
          (dropout3): Dropout(p=0.4, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (mem_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (linear1): Linear(in_features=30, out_features=20, bias=True)
          (dropout): Dropout(p=0.4, inplace=False)
          (linear2): Linear(in_features=20, out_features=30, bias=True)
          (norm1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.4, inplace=False)
          (dropout2): Dropout(p=0.4, inplace=False)
          (dropout3): Dropout(p=0.4, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (mem_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (linear1): Linear(in_features=30, out_features=20, bias=True)
          (dropout): Dropout(p=0.4, inplace=False)
          (linear2): Linear(in_features=20, out_features=30, bias=True)
          (norm1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.4, inplace=False)
          (dropout2): Dropout(p=0.4, inplace=False)
          (dropout3): Dropout(p=0.4, inplace=False)
        )
      )
      (norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
    )
    (fns): ModuleDict(
      (len_prop): Linear(in_features=30, out_features=1, bias=True)
      (rating_prop): Linear(in_features=30, out_features=1, bias=True)
      (rouge_prop): Sequential(
        (lin): Linear(in_features=30, out_features=3, bias=True)
        (sigmoid): Sigmoid()
      )
      (pov_prop): Sequential(
        (lin): Linear(in_features=30, out_features=4, bias=True)
        (softmax): Softmax(dim=-1)
      )
    )
  )
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)