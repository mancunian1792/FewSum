2022-02-22 01:17:15,376 [INFO]: CUDA_VISIBLE_DEVICES=0
2022-02-22 01:17:19,705 [INFO]: Loaded BPE from: 'artifacts/amazon/misc/bpe_32000_train.int'.
2022-02-22 01:17:19,706 [INFO]: Loading a vocabulary from 'artifacts/amazon/misc/bpe_32000_train.txt'. min_count: 1, max_vocab_size: None.
2022-02-22 01:17:19,861 [INFO]: Loaded the vocabulary.
2022-02-22 01:17:19,929 [INFO]: Vocabulary is written to: '/media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7/bpe_32000_vocab.txt'.
2022-02-22 01:17:20,633 [INFO]: Frozen all parameters except: plugin,_tr_stack.layers.0.mem_attn,_tr_stack.layers.1.mem_attn,_tr_stack.layers.2.mem_attn,_tr_stack.layers.3.mem_attn,_tr_stack.layers.4.mem_attn,_tr_stack.layers.5.mem_attn.
2022-02-22 01:17:23,883 [INFO]: Moved the model to: 'cuda'
2022-02-22 01:17:23,954 [INFO]: Loaded the model's state from: 'artifacts/amazon/checkpoints/joint_tuning.tar'.
2022-02-22 01:17:23,954 [INFO]: Experiment's output will be saved to: '/media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7'.
2022-02-22 01:17:23,955 [INFO]: 
#################################################################################
#          Joint tuning: the plug-in network and the memory attention.          #
#################################################################################

2022-02-22 01:17:23,957 [INFO]: 
###############################################################################################################################################################################################################################################
#          seed: 42, cuda_device_ids: [0], training_logging_step: 100, shuffler_buffer_size: 50                                                                                                                                               #
#          grads_clip: 0.250, strict_load: True, dataset: amazon, min_rev_per_group: 9, max_rev_per_group: 9                                                                                                                                  #
#          base_data_path: artifacts/amazon, train_fp: artifacts/amazon/reviews/train/                                                                                                                                                        #
#          val_fp: artifacts/amazon/reviews/val/, gold_train_fp: artifacts/amazon/gold_summs/train.csv                                                                                                                                        #
#          gold_val_fp: artifacts/amazon/gold_summs/val.csv, gold_test_fp: artifacts/amazon/gold_summs/test.csv                                                                                                                               #
#          word_vocab_fp: artifacts/amazon/misc/tc_word_train.txt, checkpoint_fn: checkpoint.tar                                                                                                                                              #
#          modules_to_unfreeze: ['plugin', '_tr_stack.layers.0.mem_attn', '_tr_stack.layers.1.mem_attn', '_tr_stack.layers.2.mem_attn', '_tr_stack.layers.3.mem_attn', '_tr_stack.layers.4.mem_attn', '_tr_stack.layers.5.mem_attn']          #
#          subword_num: 32000, bpe_fp: artifacts/amazon/misc/bpe_32000_train.int, bpe_vocab_fp: artifacts/amazon/misc/bpe_32000_train.txt                                                                                                     #
#          tcaser_model_path: artifacts/amazon/misc/tcaser.model, min_seq_len: 20, max_seq_len: 105                                                                                                                                           #
#          seq_max_len: 105, beam_size: 50, block_ngram_repeat: 3, ngram_mirror_window: 3                                                                                                                                                     #
#          mirror_conjs: ['and', 'or', ',', 'but'], block_consecutive: True, epochs: 33                                                                                                                                                       #
#          learning_rate: 0.000, train_groups_per_batch: 15, val_groups_per_batch: 20                                                                                                                                                         #
#          eval_groups_per_batch: 17, experiments_folder: joint_tuning, output_dir: runs/amazon/joint_tuning                                                                                                                                  #
#          output_path: /media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7                                                                                                     #
#          checkpoint_path: artifacts/amazon/checkpoints/joint_tuning.tar                                                                                                                                                                     #
###############################################################################################################################################################################################################################################

2022-02-22 01:17:23,957 [INFO]: 
#########################################################################################################################
#          word_emb_dim: 390, len_emb_num: 110, len_emb_dim: 10, word_emb_dropout: 0.100                                #
#          drop_enc_embds: True, ff_dim: 1000, dropout: 0.100, mem_att_dropout: 0.100, nheads: 8                        #
#          nlayers: 6, plugin_dim: 30, plugin_ff_dim: 20, plugin_dropout: 0.400, plugin_mem_att_dropout: 0.150          #
#          plugin_nheads: 3, plugin_nlayers: 3, word_emb_num: 33455                                                     #
#########################################################################################################################

2022-02-22 01:17:23,958 [INFO]: Trainable parameters: 3951249/25993409 (15.20 %).
2022-02-22 01:17:23,958 [INFO]: Saved config to '/media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7/run_conf.json'.
2022-02-22 01:17:23,958 [INFO]: Saved config to '/media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7/model_hp.json'.
2022-02-22 01:17:23,959 [INFO]: Performing summary evaluation on {'data_path': 'artifacts/amazon/gold_summs/test.csv'}.
2022-02-22 01:17:37,358 [INFO]: ###  ROUGE Scores  ###
2022-02-22 01:17:37,358 [INFO]:    - rouge1 p: 0.0369, r: 0.1667, f: 0.0604
2022-02-22 01:17:37,358 [INFO]:    - rouge2 p: 0.0102, r: 0.0455, f: 0.0167
2022-02-22 01:17:37,358 [INFO]:    - rougeL p: 0.0369, r: 0.1667, f: 0.0604
2022-02-22 01:17:37,360 [INFO]: Wrote the eval output to: '/media/harishramani/extradrive1/code/oogway/experiments/summarization/inference_harish/runs/amazon/joint_tuning/7/out/test_eval.json'.
2022-02-22 01:17:37,361 [INFO]: Not generated 0 summaries.
